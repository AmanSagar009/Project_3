go to root user  :hdoop
#Make sure JAVA IS INSTALLED  ..Here 11.0.11 is installed
-----------------------------------------------------------------------------------------------------------------------------------------------

$sudo apt install default-jdk scala git -y
$java -version; javac -version; scala -version; git --version
$wget https://downloads.apache.org/kafka/2.8.0/kafka_2.13-2.8.0.tgz

# or directly download from https://kafka.apache.org/downloads

$tar -xvf kafka-2.13.8.0-src.tgz              //FOR UNZIPPING THE FILE

----------------------------------------------------------------------------------------------------------------------------------------------

# Now go to  home/downloads/kafka_2.13.-2.8.0/config/server.properties    
make 2 changes in server.properties:
---1
in line 36
uncomment advertised.listeners
and edit as localhost:9092

------------------------------------------------------------------------------------------------------------------------------------------
{ go to kafka_2.13-2.8.0 directory -> ls -> cd bin }
--------------------------------------------------------------------------------------------------------------------------------------------

#START ZOOKEEPER  IN T1
bin/zookeeper-server-start.sh config/zookeeper.properties

-----------------------------------------------------------------------------------------------------------------------------------------------

#START THE SERVER  IN T2   {CTR + SHIFT + N  -> FOR NEW TERMINAL }
bin/kafka-server-start.sh config/server.properties

-----------------------------------------------------------------------------------------------------------------------------------------------

#TOPIC CREATION IN T3   {CTR + SHIFT + N  -> FOR NEW TERMINAL }
bin/kafka-topics.sh --create --zookeeper localhost:2181 --topic Projectp3 --replication-factor 1 --partitions 1
bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic Projectp3 

-----------------------------------------------------------------------------------------------------------------------------------------------

#CREATE PRODUCER  IN T4   {CTR + SHIFT + N  -> FOR NEW TERMINAL }
bin/kafka-console-producer.sh --topic Projectp3 --broker-list localhost:9092

-----------------------------------------------------------------------------------------------------------------------------------------------

#CREATE CONSUMER IN T5   {CTR + SHIFT + N  -> FOR NEW TERMINAL }
bin/kafka-console-consumer.sh --topic Projectp3 --bootstrap-server localhost:9092 --from-beginning

----------------------------------------------------------------------------------------------------------------------------------------------

list:
bin/kafka-topics.sh --list --zookeeper localhost:9092
-----------------------------------------------------------------------------------------------------------------------------------------------

Start dfs services:
start-dfs.sh
-------------------------------------------------------------------------------------------------------------------------------------------

Start yarn services:
start-yarn.sh
-------------------------------------------------------------------------------------------------------------------------------------------

Start consumer using Pyspark:

spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 kafka_streaming_json.py


What are RDDs?
RDDs or Resilient Distributed Datasets is the fundamental data structure of the Spark. It is the collection of objects which is capable of storing the data partitioned across the multiple nodes of the cluster and also allows them to do processing in parallel.

It is fault-tolerant if you perform multiple transformations on the RDD and then due to any reason any node fails. The RDD, in that case, is capable of recovering automatically.



 What are Dataframes?
It was introduced first in Spark version 1.3 to overcome the limitations of the Spark RDD. Spark Dataframes are the distributed collection of the data points, but here, the data is organized into the named columns. They allow developers to debug the code during the runtime which was not allowed with the RDDs.

Dataframes can read and write the data into various formats like CSV, JSON, AVRO, HDFS, and HIVE tables. It is already optimized to process large datasets for most of the pre-processing tasks so that we do not need to write complex functions on our own.
It uses a catalyst optimizer for optimization purposes.


What are Datasets?
Spark Datasets is an extension of Dataframes API with the benefits of both RDDs and the Datasets. It is fast as well as provides a type-safe interface. Type safety means that the compiler will validate the data types of all the columns in the dataset while compilation only and will throw an error if there is any mismatch in the data types


https://forms.office.com/pages/responsepage.aspx?id=iuJja_motUeqQJfiMSFRZF-IJT6NyVZPgv5OIv_bG2BUOVRUN1lKWUdWQTZEQkI2WjRVM1NEVzJKNS4u















